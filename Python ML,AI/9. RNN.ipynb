{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 是一種有記憶的神經網路，非常適合時間序列，或是不定長度的輸入資料。\n",
    "\n",
    "這次來用RNN做情意分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND = tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀入 IMDB 電影數據庫\n",
    "評 IMDB 電影數據庫影評"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意這裡我們限制只選最常用的一萬字，也就是超過這個範圍就當不存在。這是文字分析常會做的事。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入資料部分\n",
    "看一下輸入長什麼樣子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 25000\n",
      "Testing data: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data:\",len(x_train))\n",
    "print(\"Testing data:\",len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 17,\n",
       " 6,\n",
       " 194,\n",
       " 337,\n",
       " 7,\n",
       " 4,\n",
       " 204,\n",
       " 22,\n",
       " 45,\n",
       " 254,\n",
       " 8,\n",
       " 106,\n",
       " 14,\n",
       " 123,\n",
       " 4,\n",
       " 2,\n",
       " 270,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 732,\n",
       " 2098,\n",
       " 101,\n",
       " 405,\n",
       " 39,\n",
       " 14,\n",
       " 1034,\n",
       " 4,\n",
       " 1310,\n",
       " 9,\n",
       " 115,\n",
       " 50,\n",
       " 305,\n",
       " 12,\n",
       " 47,\n",
       " 4,\n",
       " 168,\n",
       " 5,\n",
       " 235,\n",
       " 7,\n",
       " 38,\n",
       " 111,\n",
       " 699,\n",
       " 102,\n",
       " 7,\n",
       " 4,\n",
       " 4039,\n",
       " 9245,\n",
       " 9,\n",
       " 24,\n",
       " 6,\n",
       " 78,\n",
       " 1099,\n",
       " 17,\n",
       " 2345,\n",
       " 2,\n",
       " 21,\n",
       " 27,\n",
       " 9685,\n",
       " 6139,\n",
       " 5,\n",
       " 2,\n",
       " 1603,\n",
       " 92,\n",
       " 1183,\n",
       " 4,\n",
       " 1310,\n",
       " 7,\n",
       " 4,\n",
       " 204,\n",
       " 42,\n",
       " 97,\n",
       " 90,\n",
       " 35,\n",
       " 221,\n",
       " 109,\n",
       " 29,\n",
       " 127,\n",
       " 27,\n",
       " 118,\n",
       " 8,\n",
       " 97,\n",
       " 12,\n",
       " 157,\n",
       " 21,\n",
       " 6789,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 66,\n",
       " 78,\n",
       " 1099,\n",
       " 4,\n",
       " 631,\n",
       " 1191,\n",
       " 5,\n",
       " 2642,\n",
       " 272,\n",
       " 191,\n",
       " 1070,\n",
       " 6,\n",
       " 7585,\n",
       " 8,\n",
       " 2197,\n",
       " 2,\n",
       " 2,\n",
       " 544,\n",
       " 5,\n",
       " 383,\n",
       " 1271,\n",
       " 848,\n",
       " 1468,\n",
       " 2,\n",
       " 497,\n",
       " 2,\n",
       " 8,\n",
       " 1597,\n",
       " 8778,\n",
       " 2,\n",
       " 21,\n",
       " 60,\n",
       " 27,\n",
       " 239,\n",
       " 9,\n",
       " 43,\n",
       " 8368,\n",
       " 209,\n",
       " 405,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 764,\n",
       " 40,\n",
       " 4,\n",
       " 248,\n",
       " 20,\n",
       " 12,\n",
       " 16,\n",
       " 5,\n",
       " 174,\n",
       " 1791,\n",
       " 72,\n",
       " 7,\n",
       " 51,\n",
       " 6,\n",
       " 1739,\n",
       " 22,\n",
       " 4,\n",
       " 204,\n",
       " 131,\n",
       " 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[24999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個list而非array，因為每筆資料(影評)長度是不一樣的，我們來檢查一下前10筆資料的長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1筆資料長度: 218\n",
      "第2筆資料長度: 189\n",
      "第3筆資料長度: 141\n",
      "第4筆資料長度: 550\n",
      "第5筆資料長度: 147\n",
      "第6筆資料長度: 43\n",
      "第7筆資料長度: 123\n",
      "第8筆資料長度: 562\n",
      "第9筆資料長度: 233\n",
      "第10筆資料長度: 130\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'第{i+1}筆資料長度:',len(x_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後要說的是，每筆輸入資料的數字都代表英文的一個單字，編號方式是資料庫裡所有文字的排序，出現頻率越高的字，數字代碼越小\n",
    "## 輸出資料部分\n",
    "輸出方面比較簡單，結果就是正評(1)與負評(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 送入神經網路的輸入處理\n",
    "雖然RNN可以處理不同長度的輸入，在寫程式時我們還是要\n",
    "* 設定輸入文字的長度上限\n",
    "* 把每段文字都弄成一樣長，太短的後面補0，太長的截斷\n",
    "\n",
    "因為長度一定才能將資料轉換成array的結構，方便訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=150)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 150)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開始打造RNN\n",
    "這裡選用LSTM，基本上用哪種RNN寫法都是差不多的!\n",
    "## 決定神經網路架構\n",
    "* 先將10000維的文字壓縮到N維\n",
    "* 然後用K個LSTM神經元座隱藏層\n",
    "* 最後一個output用sigmoid送出\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建構神經網路\n",
    "文字用 1-hot 表示是很標準的方式，不過要注意的是，因為我們指定要一萬個字，所以每個字是用一萬維的向量表示!\n",
    "\n",
    "這樣一來很浪費記憶空間，二來字與字間基本上是沒關係的。因此我們可以用某種合理的方式，把字壓到比較小的維度，這些向量又逮表某些意思(比如兩個字代表的向量角度小表示相關程度大)\n",
    "\n",
    "這叫做\"word embedding\"，keras會幫我們做。我們只需要告訴keras原來最大的數字是多少(10000)，以及要壓縮到幾維(N)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # 壓縮到 N 維\n",
    "K = 4 # LSTM 的神經元數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(10000,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 層，我們做 K 個 LSTM Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "單純透過 sigmoid 輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 組裝\n",
    "這次我們用 binary_crossentropy 做我們的 loss function，再加上 Adam 學習法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 3)           30000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4)                 128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 30,133\n",
      "Trainable params: 30,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM parameters = 128 = (4*7+4(bias))*(K=4)\n",
    "\n",
    "一個LSTM Neural裡面，有4個子神經元，一個做標準神經元的輸出，其他三個是管控輸出的閘門，控制閥的標準型是 sigmoid 。\n",
    "\n",
    "每個LSTM cell 有 3+4 個 input(N+隱藏層)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練\n",
    "Embedding 會被 batch_size 影響輸入，輸入的shape是(batch_size, 每筆上限)\n",
    "也就是(32,150)的輸出是(32,150,128)，其中128是我們決定要壓成幾維的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.5055 - acc: 0.7568\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 118s 5ms/step - loss: 0.2960 - acc: 0.8865\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 119s 5ms/step - loss: 0.2303 - acc: 0.9158\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 119s 5ms/step - loss: 0.1938 - acc: 0.9336\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.1661 - acc: 0.9468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1502ec9e278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,\n",
    "         batch_size=32,\n",
    "         epochs=5)  # epochs 是訓練次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 23s 928us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3757927942562103\n",
      "accuracy 0.85552\n"
     ]
    }
   ],
   "source": [
    "print('loss:',score[0])\n",
    "print('accuracy',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
